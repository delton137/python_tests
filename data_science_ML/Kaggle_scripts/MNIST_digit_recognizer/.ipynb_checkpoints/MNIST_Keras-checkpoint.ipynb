{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and do preprocessing \n",
    "First we load our data into Pandas dataframes and convert them to NumPy arrays using *.values*. We further convert the datatype from *float64* to *float32* for speed. \n",
    "\n",
    "Since the training examples are 1D vectors, and we wish to do convolutions on the 2D images, we reshape the input data from (n_train x 784) to (n_train x 28 x 28). We also normalize the data to the interval [0,1], a standard procedure which leads to larger gradients and faster training with some of the standard neural network activation functions.  \n",
    "\n",
    "We used *to_categorical* to transform the target data, which lies in the set [0,1,2,3,4,5,6,7,8,9] to one hot vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th') #input shape: (channels, height, width)\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "valid_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "x_train = train_df.drop(['label'], axis=1).values.astype('float32')\n",
    "Y_train = train_df['label'].values.astype('float32')\n",
    "x_valid = valid_df.values.astype('float32')\n",
    "\n",
    "img_width, img_height = 28, 28\n",
    "\n",
    "n_train = x_train.shape[0]\n",
    "n_valid = x_valid.shape[0]\n",
    "\n",
    "n_classes = 10 \n",
    "\n",
    "x_train = x_train.reshape(n_train,1,img_width,img_height)\n",
    "x_valid = x_valid.reshape(n_valid,1,img_width,img_height)\n",
    "\n",
    "x_train = x_train/255 #normalize from [0,255] to [0,1]\n",
    "x_valid = x_valid/255 \n",
    "\n",
    "y_train = to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View an image to make sure everything is OK\n",
    "(The images are not color, but *imshow()* applies a colormap by default, and I'm not sure how to disable it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/EBQhB8I6n8Bdc6U4MHnnnAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa3d372358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgplot = plt.imshow(x_train[4,0,:,:],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Some Parameters \n",
    "\n",
    "In this section we figure out some hyperparameters for our CNN. \n",
    "\n",
    "* Number of filters (n_filters) \n",
    "\n",
    "* Size of convolution filters (filter_size)  \n",
    "\n",
    "* Pooling window (pooling_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_filters = 64\n",
    "filter_size = 3\n",
    "pooling_size = 2\n",
    "n_dense = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model \n",
    "\n",
    "** Handling edges/borders **  \n",
    "One thing we have to decide is how to deal with the edges. To allow convolution of the data at the edges, one can first 'zero pad' the input array, by adding zeros to the left, right, top, and bottom. ie:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;00000<br>\n",
    "123&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;01230<br>\n",
    "456&nbsp;-->&nbsp;04560<br>\n",
    "789&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;07890<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;00000<br> \n",
    "\n",
    "\n",
    "This can be done with the [ZeroPadding2D()](https://keras.io/layers/convolutional/#ZeroPadding2D) function in Keras. One must make sure to zero pad with enough zeros -- one needs *filter_size/2* zeros. \n",
    "\n",
    "Alternatively, a simpler solution is to set *border_mode='same'*, which returns a filter map of the same size. \n",
    "\n",
    "The other option available in Keras is *border_mode='valid'* which only does convolutions where the filter fits inside the image (also called narrow convolution). With this option set, the filter map has smaller dimensions than the input image. \n",
    "\n",
    "**[2D Convolution](https://keras.io/layers/convolutional/#convolution2d)**  \n",
    "The main operation in the CNN. \n",
    "\n",
    "\n",
    "**[Max Pooling](https://keras.io/layers/pooling/#maxpooling2d)**  \n",
    "Max pooling reduces the size of the filter maps, by applying a *max filter* to non-overlapping subregions. A max pooling layer with pooling_size=2 (ie using 2x2 max filters) will reduce the number total number of parameters in the filter map by a factor of 4.\n",
    "\n",
    "**[Dropout](https://keras.io/layers/core/#dropout)**  \n",
    "This is a technique for preventing overfitting. The dropout layer in Keras randomly drops a certain fraction of the neurons (units) with a probability p in each training round. This forces the network to learn redundant representations, and effectively lowers the number of paramters while maintaining a wide degree of flexibility.\n",
    "\n",
    "**[Flattening](https://keras.io/layers/core/#flaten)**  \n",
    "Flattening converts the input activations, which are in an array of shape (n_filters, filter_size_x, filter_size_y) into a 1D array. \n",
    "\n",
    "**[Dense layer](https://keras.io/layers/core/#dense)**  \n",
    "This is a fully connected layer of neurons which works on the 1D input and gives a 1D output. \n",
    "\n",
    "**[Softmax activation](https://en.wikipedia.org/wiki/Softmax_function#Artificial_neural_networks)**  \n",
    "Softmax converts the input  \n",
    "\n",
    "### Compilation  \n",
    "The 'compilation' step is where you specify your loss function in Keras. In this case, we use categorical crossentropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), batch_input_shape=(None, 1, ..., activation=\"relu\", padding=\"valid\")`\n",
      "/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (2, 2), activation=\"relu\", padding=\"valid\")`\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import Dropout, Dense, Flatten, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(n_filters, 3, 3, batch_input_shape=(None, 1, img_width, img_height), activation='relu', border_mode='valid'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Convolution2D(n_filters, 2, 2, activation='relu', border_mode='valid'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "see *[fit()](https://keras.io/models/sequential/#fit)* documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.6/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37800 samples, validate on 4200 samples\n",
      "Epoch 1/50\n",
      " 2500/37800 [>.............................] - ETA: 138s - loss: 0.1727 - acc: 0.9480\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 1\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=n_epochs,verbose=1, validation_split=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on validation data and save output\n",
    "see *[predict_classes()](https://keras.io/models/sequential/#predict_classes)* documentation. \n",
    "\n",
    "(By contrast, *[predict()](https://keras.io/models/sequential/#predict)*  would return an array with shape (n_examples, n_classes), where each number represents a probability for the class in question.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yPred = model.predict_classes(x_valid,batch_size=32,verbose=1)\n",
    "\n",
    "np.savetxt('mnist_output.csv', np.c_[range(1,len(yPred)+1),yPred], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
